{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré processamento: \n",
    "Funções do arquivo pre_processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_processing as pp\n",
    "import analysis as anl\n",
    "import pca\n",
    "\n",
    "category = 'dvd'\n",
    "\n",
    "hNeg = True #if true, add negative bigrams for negative reviews\n",
    "noun = False #if true, add nouns\n",
    "\n",
    "X, Y, vocabulary = pp.bow(category, hNeg, noun)\n",
    "\n",
    "print(\"Vocabulário possui \" + str(len(vocabulary)) + \" palavras!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "X2, Y2 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "#X2, Y2 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "pTrain = 0.8\n",
    "\n",
    "train_index, test_index = anl.stratified_holdOut(Y, pTrain)\n",
    "\n",
    "Xtrain, Xval = X2[train_index, :], X2[test_index, :]\n",
    "Ytrain, Yval = Y2[train_index], Y2[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, new_vocabulary, index = pp.chi2(Xtrain, Ytrain, vocabulary)\n",
    "Xval = Xval[:, index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de features antes do chi-quadrado: \" + str(len(vocabulary)))\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Número de features após chi-quadrado: \" + str(len(new_vocabulary)))\n",
    "print(new_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13898337  0.24846909 -0.04268368  0.29877234  0.3699251   0.33709823\n",
      "  0.10509407  0.26248738 -0.12346596  0.18930471  0.22022051  0.16972507\n",
      "  0.00387098  0.01614324 -0.25940445  0.66232977  0.09772579  0.20449163\n",
      "  0.40114565 -0.02199044 -0.04938454 -0.0601969   0.62015459  0.07420372]\n"
     ]
    }
   ],
   "source": [
    "def gradienteDescente(X, Y, theta, alpha, m, num_iter):\n",
    "\n",
    "    for it in range(num_iter):\n",
    "        h_theta = (X * theta).sum(axis=1)\n",
    "        theta = theta - alpha * (1/m) *(X.T * (h_theta - Y)).sum(axis=1)\n",
    "      \n",
    "    return theta\n",
    "\n",
    "numIterations = 100\n",
    "alpha = 0.55\n",
    "m,n = np.shape(Xtrain)\n",
    "theta = np.ones(n)\n",
    "theta = gradienteDescente(Xtrain, Ytrain, theta, alpha, m, numIterations)\n",
    "print(theta)\n",
    "# acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "# print(acuracia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia é 0.6875\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "classes = clf.predict(Xval)\n",
    "acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Florest Nossa implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed May  9 23:20:03 2018\n",
    "Decision Tree classifier, used to classify datasets with any number of continuous attributes.\n",
    "@author:Samuel Oswald\n",
    "\"\"\"\n",
    "##Import numpy for management of arrays.\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Build decision tree. data refers to the training dataset.\n",
    "max_depth refers to how deep the tree can get. min_size is the minimum\n",
    "amount of samples before a leaf node must be classified.\"\"\"\n",
    "def dt_train( data, max_depth, min_size = 1):\n",
    "    max_depth = int(max_depth)\n",
    "    min_size = int(min_size)\n",
    "    attr, split_val, left, right = split(data)\n",
    "    tree = {\"attribute\": attr, \"split\": split_val, \"left\": left, \"right\": right, \"current_mode\": leaf(data)}\n",
    "    decision(tree,max_depth,min_size)\n",
    "    return tree\n",
    "\n",
    "def gini(node):\n",
    "    \"\"\"Calculate the gini impurity for a node. Aim is to minimize gini impurity(gain function).\"\"\"\n",
    "    ##Find the number of classifications in current node.\n",
    "    classifications = node[:,-1]\n",
    "    samples = classifications.size\n",
    "    unique, counts = np.unique(classifications, return_counts = True)\n",
    "    ##calculate gini based on number of classes\n",
    "    gini = 1\n",
    "    for i in range (0, unique.size):\n",
    "        proportion =  counts[i] / samples\n",
    "        ##\n",
    "        gini = gini - proportion * proportion\n",
    "    return gini\n",
    "    \n",
    "def gain(values, cur_gini, attribute, split):\n",
    "    \"\"\"Calculate information gain for an attribute split at each level.\n",
    "    Inputs are the current subset of data, initial gini at parent node,\n",
    "    attribute to be split and split number.\"\"\"\n",
    "    i = attribute\n",
    "    samples = values[:,-1].size\n",
    "    left = values[values[:,i] < split, :]\n",
    "    right = values[values[:,i] >= split, :]\n",
    "    left_samples = left[:,-1].size\n",
    "    right_samples = right[1:,-1].size\n",
    "    \n",
    "    ##Calculate left and right side gini\n",
    "    left_gini = gini(left)\n",
    "    right_gini = gini(right)\n",
    "    \n",
    "    ##Calculate information gain at this split value.\n",
    "    gain = cur_gini - (left_samples/samples)*left_gini - (right_samples/samples)*right_gini\n",
    "    return gain, left, right\n",
    "    \n",
    "def split(node):\n",
    "    \"\"\"Find the ideal split point by searching for the best information gain\n",
    "    of all attributes and their potential split values.\n",
    "    If no gain improves, node is split for leaf node creation as right side left at 0 samples.\"\"\"\n",
    "    cur_gini = gini(node)\n",
    "    best_gain = 0\n",
    "    best_attr = 0\n",
    "    best_split = 0\n",
    "    ##Implement greedy, exhaustive search for best information gain\n",
    "    variables = len(node[0])\n",
    "    best_left = node\n",
    "    best_right = np.empty([0,variables])\n",
    "    \n",
    "    ##Seach through each unique value to find best division\n",
    "    for v in range(0, variables-1):\n",
    "        uniques = np.unique(node[:, v])\n",
    "        for row in uniques:\n",
    "            new_gain, left, right  = gain(node, cur_gini, v, row)\n",
    "            \n",
    "            ##Select the best gain, and associated attributes\n",
    "            if new_gain > best_gain:\n",
    "                best_gain = new_gain\n",
    "                best_attr = v\n",
    "                best_split = row\n",
    "                best_left = left\n",
    "                best_right = right\n",
    "    #return {\"attribute\": best_attr, \"split\": best_split, \"left\": best_left, \"right\": best_right}\n",
    "    return best_attr, best_split, best_left, best_right\n",
    "\n",
    "def leaf(node):\n",
    "    \"\"\"Return classification value for leaf node, \n",
    "    when either maximum depth of tree reached or node is suitably weighted to one class.\"\"\"\n",
    "    classes = node[:, -1].tolist()\n",
    "    return max(set(node[:,-1]), key = classes.count)\n",
    "\n",
    "def decision(tree, max_depth=10, min_size=0, depth=0):\n",
    "    \"\"\"Uses split and leaf functions to build a tree, using a root data set.\n",
    "    Will assign leaf nodes if either maximum depth or minimum samples are reached.\n",
    "    root node contains both current node data, as well as decision rules to that point.\n",
    "    \"\"\"\n",
    "    left = tree[\"left\"]\n",
    "    right = tree[\"right\"]\n",
    "      \n",
    "    ##If tree is at max depth, assign most common member.\n",
    "    if depth >= max_depth:\n",
    "        tree['left'] = leaf(left)\n",
    "        tree['right'] = leaf(right)\n",
    "    ##If continuing sampling\n",
    "    else:\n",
    "        \n",
    "        ##Left side child\n",
    "        ##If minimum samples exist in current node, make it a leaf with max occuring value in samples.\n",
    "        if left[:, -1].size <= min_size:\n",
    "            tree['left'] = leaf(left)\n",
    "        ##Else continue building tree.\n",
    "        else:\n",
    "            left_attr, left_split, left_left, left_right = split(left)\n",
    "            ##Check if node is terminal. Make it a leaf node if so.\n",
    "            if left_left.size == 0 or left_right.size == 0:\n",
    "                tree['left'] = leaf(np.vstack([left_left,left_right]))   \n",
    "            ##Continue elsewise.\n",
    "            else:\n",
    "                tree['left'] = {\"attribute\": left_attr, \"split\": left_split, \"left\": left_left, \"right\": left_right, \"current_mode\": leaf(left)}\n",
    "                decision(tree['left'], max_depth, min_size, depth+1)\n",
    "                \n",
    "        ##right side child. Same process as above.\n",
    "        if right[:, -1].size <= min_size:\n",
    "            tree['right'] = leaf(right)\n",
    "        else:\n",
    "            right_attr, right_split, right_left, right_right = split(right)\n",
    "            if right_left.size == 0 or right_right.size == 0:\n",
    "                tree['right'] = leaf(np.vstack([right_left,right_right]))\n",
    "            else:\n",
    "                tree['right'] = {\"attribute\": right_attr, \"split\": right_split, \"left\": right_left, \"right\": right_right, \"current_mode\": leaf(right)}\n",
    "                decision(tree['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "def classify(tree,row):\n",
    "    \"\"\"classify new data based on current row.\n",
    "    Involves searching through tree based on the attributes of validation data.\n",
    "    Will return classification value once leaf of tree is reached.\"\"\"\n",
    "    ##Look at each sample to classify. append to list of output values.\n",
    "    ##Recursively search through branches until an append can be made.\n",
    "    if row[tree['attribute']] < tree['split']:\n",
    "        if isinstance(tree['left'],dict):\n",
    "            return classify(tree['left'], row)\n",
    "        else:\n",
    "            return tree['left']\n",
    "    else:\n",
    "        if isinstance(tree['right'],dict):\n",
    "            return classify(tree['right'], row)\n",
    "        else:\n",
    "            return tree['right']\n",
    "\n",
    "def dt_predict( tree, data):\n",
    "    \"\"\"For every row in the validation data,\n",
    "    a call to the classify function is done,\n",
    "    with results appended to prediction data.\"\"\"\n",
    "    predictions = []\n",
    "    for row in data:\n",
    "        pred = classify(tree, row)\n",
    "        predictions.append(int(pred))\n",
    "    return predictions\n",
    "\n",
    "##functions for validation and pruning.\n",
    "def dt_confusion_matrix( predicted, actual,classes):\n",
    "    \"\"\"Return a confusion matrix showing the difference between actual values,\n",
    "    and model predicted values. Also returns total accuracy\"\"\"\n",
    "    \n",
    "    matrix = np.zeros((len(classes), len(classes)))\n",
    "    for a, p in zip(actual, predicted):\n",
    "        matrix[a][p] += 1\n",
    "    accuracy = (actual == predicted).sum() / float(len(actual))*100\n",
    "    return matrix, accuracy        \n",
    "    \n",
    "def print_dt(tree, depth = 0):\n",
    "    \"\"\"\"Iterate through decision tree, printing out values.\"\"\"\n",
    "    print ((\" \" * depth) + \"attribute \" + str(tree['attribute']) + \" > \" + str(tree['split']))\n",
    "    if isinstance(tree['left'], dict):\n",
    "        print_dt(tree['left'], depth + 1)\n",
    "    else:\n",
    "        print ((\" \" *(depth + 1)) + str(tree['left']))\n",
    "    if isinstance(tree['right'], dict):\n",
    "        print_dt(tree['right'], depth + 1)\n",
    "    else:\n",
    "        print ((\" \" *(depth + 1)) + str(tree['right']))\n",
    "        \n",
    "\n",
    "\"\"\"Bagged decision trees contain a user-specified number of decision trees.\n",
    "Classification of a sample is done by using the mode of each of these decision trees.\n",
    "subsample is a fraction of the total dataset to be used.\n",
    "trees refers to the number of trees to use in \"forest\" of trees.\n",
    "By leaving default values for subsample and trees, a single decision tree classifier is created.\"\"\"\n",
    "def bt_train( data, max_depth, min_size = 1, subsample_ratio = 1,trees =1):\n",
    "    \n",
    "    ##Create a series of trees using sampling with replacement.\n",
    "    size = data[:, -1].size\n",
    "    division = int(size * subsample_ratio)        \n",
    "    forest = []\n",
    "    for i in range (0,trees):\n",
    "        samples = data[np.random.choice(data.shape[0], division, replace = True)]\n",
    "        forest.append([])\n",
    "        forest[i] = dt_train(samples, max_depth, min_size)\n",
    "    return forest\n",
    "\n",
    "def bt_predict( forest, data):\n",
    "    \"\"\"\"Classify validation data set based on built bagged trees.\n",
    "    This is done by taking the mode of the classifications of each decision tree.\"\"\"\n",
    "    ##Use predict function from decision tree.\n",
    "    ##Number of trees in forest, number of validation samples. Used to create empty array showing classifications.\n",
    "    forest_size = len(forest)\n",
    "    samples = len(data)\n",
    "    tree_classification = np.zeros((samples, forest_size))\n",
    "    ##With each tree, find the classification of each validation sample.\n",
    "    for i in range (0, forest_size):\n",
    "        tree_classification[:, i] = dt_predict(forest[i], data)\n",
    "    ##Create list of modes for each sample, using tree_classification matrix.\n",
    "    predictions = []\n",
    "    for i in range(0, samples):\n",
    "        tree_pred = tree_classification[i,:].tolist()\n",
    "        predictions.append(int(max(set(tree_pred), key = tree_pred.count)))\n",
    "    return predictions\n",
    "\n",
    "def bt_confusion_matrix( predicted, actual,classes):\n",
    "    \"\"\"Create confusion matrix for bagged trees. Makes call to DT method.\"\"\"\n",
    "    matrix, accuracy = dt_confusion_matrix(predicted, actual, classes)\n",
    "    return matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "finish train\n",
      "68.75\n"
     ]
    }
   ],
   "source": [
    "classes = [0, 1]\n",
    "train = np.column_stack((Xtrain ,Ytrain))\n",
    "\n",
    "test = np.column_stack((Xval ,Yval))\n",
    "print(\"start train\")\n",
    "tree = dt_train(train, 20,5)\n",
    "print(\"finish train\")\n",
    "validation_dt = dt_predict(tree, test)\n",
    "confusion_dt,accuracy_dt = dt_confusion_matrix(validation_dt, test[:, -1].astype(int), classes)\n",
    "print (accuracy_dt)\n",
    "#print (print_dt(tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n",
      "finish train\n",
      "75.0\n"
     ]
    }
   ],
   "source": [
    "print(\"start train\")\n",
    "forest = bt_train(train, 20, 5, 1, 100)\n",
    "print(\"finish train\")\n",
    "validation_rf = bt_predict(forest, test)\n",
    "confusion_rf, accuracy_rf = bt_confusion_matrix(validation_rf, test[:, -1].astype(int),classes)\n",
    "print (accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVENTAR +++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
