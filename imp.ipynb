{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré processamento: \n",
    "Funções do arquivo pre_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pre_processing as pp\n",
    "import analysis as anl\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "category = 'dvd'\n",
    "vocabulary = []\n",
    "X, Y, vocabulary = pp.bow(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "X2, Y2 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "#X2, Y2 = X[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "pTrain = 0.8\n",
    "\n",
    "train_index, test_index = anl.stratified_holdOut(Y, pTrain)\n",
    "\n",
    "Xtrain, Xval = X2[train_index, :], X2[test_index, :];\n",
    "Ytrain, Yval = Y2[train_index], Y2[test_index];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pursu', 'weird', 'underr', 'highest', 'nope', 'tradit', 'score', 'expert', 'absurd', ('not', 'big'), 'tomlin', 'atmospher', 'anniversari', 'eli', 'fool', ('not', 'bother'), 'devast', 'richard', 'usa', 'search', 'vapid', 'huge', 'support', 'wast', 'mix', 'potenti', ('never', 'read'), ('not', 'wast'), 'bother', 'fantast', 'overr', 'throw', 'vocal', ('not', 'quit'), 'hilari', 'learn', 'speak', 'sea', 'stupid', 'disappoint', 'hot', 'gothic', ('not', 'worth'), 'instead', ('not', 'said'), 'comic', 'awesom', 'weak', 'sadli', 'cheat', 'confirm', 'heard', 'wander', 'take', 'underappreci', 'america', 'futurist', 'closet', 'raunchi', 'candl', 'unforgett', 'attribut', 'slightest', 'predict', 'blame', 'expect', 'uniqu', 'tear', 'ten', 'lynch', 'mean', 'confus', 'academi', 'song', 'felt', 'chose', 'scare', 'seem', 'eventu', 'nonsens', ('not', 'worst'), 'mani', 'recommend', 'home', 'allow', 'wherea', 'manipul', 'appear', 'uninvolv', 'electr', 'privat', 'bare', 'forgett', 'tepid', 'jare', 'nowaday', 'first', 'solv', 'beauti', ('not', 'last'), 'greatest', 'introduc', 'woo', 'moral', 'realist', 'fresh', 'latter', 'geniu', 'elia', 'barri', 'fairi', 'violent', 'trim', 'paid', 'pornograph', 'advis', 'base', 'unexpect', 'new', 'mention', 'dismal', 'alfr', 'rear', 'replay', 'refresh', 'dumb', 'name', 'act', 'drawn', 'talent', 'seamlessli', 'steven', ('not', 'alway'), 'outstand', 'walk', 'wors', 'wonder', 'find', 'afraid', 'face', 'alright', 'featur', 'rivet', 'empti', 'exact', 'novic', 'daili', 'remot', 'southern', 'cold', 'masterpiec', ('not', 'care'), 'serv', 'highli', 'structur', 'senior', 'perfect', 'stage', 'date', 'classic', 'pretenti', 'sweet', 'broken', 'laughabl', 'appar', 'compar', 'helen', 'particip', 'will', 'bowl', 'favorit', 'commit', 'slowli', 'excus', 'ridicul', 'well', 'steal', 'box', 'intens', 'unfair', 'amaz', 'equal', 'utterli', 'rare', 'true', 'enjoy', 'physic', ('not', 'good'), 'vol', 'aw', 'forth', 'insipid', 'splendid', 'mayb', 'seek', 'suppos', 'rent', 'audibl', 'otherwis', 'popular', 'bigger', 'heaven', ('not', 'mani'), 'unfortun', 'brilliantli', 'fail', 'laid', 'poor', 'commentari', 'shortli', 'person', 'solid', 'advertis', 'purchas', 'feminin', 'crap', 'save', 'struggl', 'hostil', 'spiritu', 'surpris', 'color', 'thrill', 'rental', 'entertain', 'terrif', 'differ', 'insight', 'lame', 'ray', 'marri', 'fair', 'evolv', 'unbeliev', 'except', 'wayn', 'pathet', 'actress', 'left', 'elmo', 'haunt', 'doc', 'aid', 'stalk', 'plan', 'lack', 'nostalgia', 'worst', 'experi', 'unfunni', 'abl', ('not', 'understand'), 'ensembl', 'superb', 'best', 'awar', 'asian', 'fabul', 'includ', 'captiv', 'lucki', 'fell', 'gorgeou', 'alex', 'susan', 'pass', 'eve', 'sceen', 'tediou', 'monoton', 'becom', 'step', ('not', 'found'), 'golden', 'cheap', 'evid', 'fight', 'hyster', 'creepi', ('never', 'work'), 'torment', 'insult', 'islam', 'sublim', 'give', 'redeem', 'protect', 'asleep', 'frankli', 'brought', 'crash', ('not', 'show'), 'histor', 'atroci', 'els', 'think', 'billi', 'pride', 'team', 'yet', 'better', 'tire', 'screenplay', 'grace', 'tale', 'teacher', 'love', 'passion', 'great', 'suffer', 'unforgiv', 'snow', 'burn', 'prepar', 'extra', 'historian', 'mysteri', 'next', 'tour', 'keen', 'gotten', 'butt', 'star', 'half', 'choos', 'grew', 'top', 'frustrat', 'worn', 'strike', 'join', 'sinatra', 'cook', 'freeman', 'impress', 'older', 'brilliant', 'bore', 'least', 'la', 'existenti', 'father', 'liven', 'unpleas', ('not', 'pleas'), 'sometim', 'corpor', 'music', 'vs', 'away', 'mindless', ('not', 'throw'), 'soon', 'thin', 'teeth', 'ahead', 'young', 'arrest', ('not', 'disappoint'), 'warm', ('not', 'recal'), 'watchabl', 'basic', 'ride', 'wooden', 'delici', 'teach', 'hook', 'photograph', 'actual', ('not', 'love'), 'read', 'bitter', 'touch', 'rest', 'trite', 'alway', 'recal', 'deep', 'chill', 'illustr', 'stick', 'soft', 'stay', 'add', 'rose', 'poetic', 'rubbish', 'head', 'famou', 'annoy', 'morgan', 'break', 'unexplain', 'unconvinc', 'theatric', 'meant', 'superbl', 'horrid', 'lui', 'question', 'offend', 'offens', 'suffici', 'dull', 'strong', 'play', 'forgotten', 'fonda', 'dean', 'regret', 'drew', 'expertli', 'contriv', 'indic', ('not', 'hold'), 'popcorn', 'safe', 'buy', 'second', 'smart', 'small', 'magnific', 'also', 'endless', 'amazon', ('not', 'happi'), 'flick', 'larg', 'led', 'emot', 'flash', 'hell', 'ruin', 'exactli', 'last', 'object', 'heartwarm', ('not', 'actual'), 'nobodi', 'rang', 'regular', 'recov', 'appreci', 'wong', ('not', 'get'), 'andi', 'howev', 'recogn', 'fit', 'constantli', 'unfold', 'horrend', 'bring', 'sum', 'immort', 'pearl', 'stiff', 'horribl', 'expens', 'local', 'select', 'agenda', 'stun', 'regard', 'sneak', ('not', 'believ'), ('not', 'great'), 'distract', 'summari', 'funnier', 'finest', 'power', 'legendari', 'poorli', 'friendship', 'bodi', 'regardless', ('not', 'regret'), 'laugh', 'attack', ('not', 'mean'), 'racist', 'infatu', 'silli', ('not', 'read'), 'martin', 'radic', 'episod', 'silent', 'clip', 'inevit', 'unwatch', 'incomprehens', ('not', 'recommend'), 'manner', 'human', ('not', 'even'), 'suspens', 'titl', 'not', 'terrorist', 'invit', 'vastli', ('not', 'work'), 'ethic', 'guy', 'combin', 'comfort', 'feast', 'youngest', 'retard', 'bear', 'nomin', 'item', 'repetit', 'heavi', 'win', 'hayden', 'dark', 'ii', ('not', 'engag'), 'old', 'particular', 'respond', ('not', 'buy'), 'skip', 'fulli', 'thought', 'test', 'discov', 'cut', 'offici', 'ta', 'control', 'anxiou', 'soviet', 'pleasur', 'marvel', 'deliv', 'proud', 'gem', 'flat', 'doubl', 'embarass', 'especi', 'aim', 'piti', 'ador', 'unorigin', 'c', 'probabl', 'experienc', ('not', 'fall'), 'tell', 'elimin', 'care', 'asham', 'opposit', 'poignant', 'west', 'oscar', 'epic', 'angl', ('not', 'stop'), 'infecti', 'grow', 'offer', 'tough', 'present', 'shaki', 'anyway', 'anonym', 'edit', 'quit', 'crappi', 'cheesi', 'tune', 'remind', 'focu', 'magic', ('not', 'make'), 'guest', 'refus', 'uninspir', 'ferri', 'fourth', 'adventur', 'unrealist', 'want', 'down', 'freeli', 'host', 'suck', 'sharp', 'sorri', 'twin', 'took', 'pointless', 'arriv', 'arrog', 'son', 'kill', 'develop', 'owe', 'militari', 'affair', 'terribl', 'ryan', 'sam', 'seinfeld', 'kid', 'mediocr', 'see', 'enhanc', ('not', 'much'), 'owen', 'mark', 'almost', ('not', 'funni'), 'fun', 'previou', 'soul', 'said', 'nauseat', 'relax', 'warner', 'deni', 'whatsoev', 'extraordinari', 'cowboy', 'contain', 'ok', 'full', 'intact', 'strang', 'still', 'scott', 'slap', 'excel', 'grossli', ('not', 'save'), 'receiv', 'transcend', ('not', 'think'), 'view', 'uninterest', 'central', 'wood', 'slow', 'nice', 'cool', 'uncomfort', 'tommi', 'wrestl', 'bad', 'compos', 'hard', 'creat', 'rave', 'dare', 'subtl']\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.2\n",
    "Xtrain, new_vocabulary, selected = pp.select_c2(Xtrain, Ytrain, vocabulary, alpha)\n",
    "print(new_vocabulary)\n",
    "Xval = Xval[:, selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout estratificado para separar dados para treino e teste:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.208978\n",
      "         Iterations: 50\n",
      "         Function evaluations: 52\n",
      "         Gradient evaluations: 52\n",
      "Acurácia é 56.25\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import regressao_logistica as rl\n",
    "\n",
    "acuracia = rl.regressao_logistica(Xtrain,Ytrain,Xval,Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia é 0.39\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import redes_neurais as rn\n",
    "\n",
    "acuracia = rn.redes_neurais (Xtrain,Ytrain,Xval,Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26248738 -0.12346596  0.07420372  0.01614324 -0.13898337  0.40114565\n",
      "  0.10509407  0.16972507  0.3699251   0.29877234  0.20449163 -0.04938454\n",
      "  0.22022051  0.24846909  0.66232977  0.33709823  0.09772579 -0.02199044\n",
      "  0.00387098  0.62015459 -0.0601969  -0.25940445  0.18930471 -0.04268368]\n"
     ]
    }
   ],
   "source": [
    "def gradienteDescente(X, Y, theta, alpha, m, num_iter):\n",
    "\n",
    "    for it in range(num_iter):\n",
    "        h_theta = (X * theta).sum(axis=1)\n",
    "        theta = theta - alpha * (1/m) *(X.T * (h_theta - Y)).sum(axis=1)\n",
    "      \n",
    "    return theta\n",
    "\n",
    "numIterations = 100\n",
    "alpha = 0.55\n",
    "m,n = np.shape(Xtrain)\n",
    "theta = np.ones(n)\n",
    "theta = gradienteDescente(Xtrain, Ytrain, theta, alpha, m, numIterations)\n",
    "print(theta)\n",
    "# acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "# print(acuracia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia é 0.7375\n"
     ]
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "classes = clf.predict(Xval)\n",
    "acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Florest Nossa implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed May  9 23:20:03 2018\n",
    "Decision Tree classifier, used to classify datasets with any number of continuous attributes.\n",
    "@author:Samuel Oswald\n",
    "\"\"\"\n",
    "##Import numpy for management of arrays.\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Build decision tree. data refers to the training dataset.\n",
    "max_depth refers to how deep the tree can get. min_size is the minimum\n",
    "amount of samples before a leaf node must be classified.\"\"\"\n",
    "def dt_train( data, max_depth, min_size = 1):\n",
    "    max_depth = int(max_depth)\n",
    "    min_size = int(min_size)\n",
    "    attr, split_val, left, right = split(data)\n",
    "    tree = {\"attribute\": attr, \"split\": split_val, \"left\": left, \"right\": right, \"current_mode\": leaf(data)}\n",
    "    decision(tree,max_depth,min_size)\n",
    "    return tree\n",
    "\n",
    "def gini(node):\n",
    "    \"\"\"Calculate the gini impurity for a node. Aim is to minimize gini impurity(gain function).\"\"\"\n",
    "    ##Find the number of classifications in current node.\n",
    "    classifications = node[:,-1]\n",
    "    samples = classifications.size\n",
    "    unique, counts = np.unique(classifications, return_counts = True)\n",
    "    ##calculate gini based on number of classes\n",
    "    gini = 1\n",
    "    for i in range (0, unique.size):\n",
    "        proportion =  counts[i] / samples\n",
    "        ##\n",
    "        gini = gini - proportion * proportion\n",
    "    return gini\n",
    "    \n",
    "def gain(values, cur_gini, attribute, split):\n",
    "    \"\"\"Calculate information gain for an attribute split at each level.\n",
    "    Inputs are the current subset of data, initial gini at parent node,\n",
    "    attribute to be split and split number.\"\"\"\n",
    "    i = attribute\n",
    "    samples = values[:,-1].size\n",
    "    left = values[values[:,i] < split, :]\n",
    "    right = values[values[:,i] >= split, :]\n",
    "    left_samples = left[:,-1].size\n",
    "    right_samples = right[1:,-1].size\n",
    "    \n",
    "    ##Calculate left and right side gini\n",
    "    left_gini = gini(left)\n",
    "    right_gini = gini(right)\n",
    "    \n",
    "    ##Calculate information gain at this split value.\n",
    "    gain = cur_gini - (left_samples/samples)*left_gini - (right_samples/samples)*right_gini\n",
    "    return gain, left, right\n",
    "    \n",
    "def split(node):\n",
    "    \"\"\"Find the ideal split point by searching for the best information gain\n",
    "    of all attributes and their potential split values.\n",
    "    If no gain improves, node is split for leaf node creation as right side left at 0 samples.\"\"\"\n",
    "    cur_gini = gini(node)\n",
    "    best_gain = 0\n",
    "    best_attr = 0\n",
    "    best_split = 0\n",
    "    ##Implement greedy, exhaustive search for best information gain\n",
    "    variables = len(node[0])\n",
    "    best_left = node\n",
    "    best_right = np.empty([0,variables])\n",
    "    \n",
    "    ##Seach through each unique value to find best division\n",
    "    for v in range(0, variables-1):\n",
    "        uniques = np.unique(node[:, v])\n",
    "        for row in uniques:\n",
    "            new_gain, left, right  = gain(node, cur_gini, v, row)\n",
    "            \n",
    "            ##Select the best gain, and associated attributes\n",
    "            if new_gain > best_gain:\n",
    "                best_gain = new_gain\n",
    "                best_attr = v\n",
    "                best_split = row\n",
    "                best_left = left\n",
    "                best_right = right\n",
    "    #return {\"attribute\": best_attr, \"split\": best_split, \"left\": best_left, \"right\": best_right}\n",
    "    return best_attr, best_split, best_left, best_right\n",
    "\n",
    "def leaf(node):\n",
    "    \"\"\"Return classification value for leaf node, \n",
    "    when either maximum depth of tree reached or node is suitably weighted to one class.\"\"\"\n",
    "    classes = node[:, -1].tolist()\n",
    "    return max(set(node[:,-1]), key = classes.count)\n",
    "\n",
    "def decision(tree, max_depth=10, min_size=0, depth=0):\n",
    "    \"\"\"Uses split and leaf functions to build a tree, using a root data set.\n",
    "    Will assign leaf nodes if either maximum depth or minimum samples are reached.\n",
    "    root node contains both current node data, as well as decision rules to that point.\n",
    "    \"\"\"\n",
    "    left = tree[\"left\"]\n",
    "    right = tree[\"right\"]\n",
    "      \n",
    "    ##If tree is at max depth, assign most common member.\n",
    "    if depth >= max_depth:\n",
    "        tree['left'] = leaf(left)\n",
    "        tree['right'] = leaf(right)\n",
    "    ##If continuing sampling\n",
    "    else:\n",
    "        \n",
    "        ##Left side child\n",
    "        ##If minimum samples exist in current node, make it a leaf with max occuring value in samples.\n",
    "        if left[:, -1].size <= min_size:\n",
    "            tree['left'] = leaf(left)\n",
    "        ##Else continue building tree.\n",
    "        else:\n",
    "            left_attr, left_split, left_left, left_right = split(left)\n",
    "            ##Check if node is terminal. Make it a leaf node if so.\n",
    "            if left_left.size == 0 or left_right.size == 0:\n",
    "                tree['left'] = leaf(np.vstack([left_left,left_right]))   \n",
    "            ##Continue elsewise.\n",
    "            else:\n",
    "                tree['left'] = {\"attribute\": left_attr, \"split\": left_split, \"left\": left_left, \"right\": left_right, \"current_mode\": leaf(left)}\n",
    "                decision(tree['left'], max_depth, min_size, depth+1)\n",
    "                \n",
    "        ##right side child. Same process as above.\n",
    "        if right[:, -1].size <= min_size:\n",
    "            tree['right'] = leaf(right)\n",
    "        else:\n",
    "            right_attr, right_split, right_left, right_right = split(right)\n",
    "            if right_left.size == 0 or right_right.size == 0:\n",
    "                tree['right'] = leaf(np.vstack([right_left,right_right]))\n",
    "            else:\n",
    "                tree['right'] = {\"attribute\": right_attr, \"split\": right_split, \"left\": right_left, \"right\": right_right, \"current_mode\": leaf(right)}\n",
    "                decision(tree['right'], max_depth, min_size, depth+1)\n",
    "\n",
    "def classify(tree,row):\n",
    "    \"\"\"classify new data based on current row.\n",
    "    Involves searching through tree based on the attributes of validation data.\n",
    "    Will return classification value once leaf of tree is reached.\"\"\"\n",
    "    ##Look at each sample to classify. append to list of output values.\n",
    "    ##Recursively search through branches until an append can be made.\n",
    "    if row[tree['attribute']] < tree['split']:\n",
    "        if isinstance(tree['left'],dict):\n",
    "            return classify(tree['left'], row)\n",
    "        else:\n",
    "            return tree['left']\n",
    "    else:\n",
    "        if isinstance(tree['right'],dict):\n",
    "            return classify(tree['right'], row)\n",
    "        else:\n",
    "            return tree['right']\n",
    "\n",
    "def dt_predict( tree, data):\n",
    "    \"\"\"For every row in the validation data,\n",
    "    a call to the classify function is done,\n",
    "    with results appended to prediction data.\"\"\"\n",
    "    predictions = []\n",
    "    for row in data:\n",
    "        pred = classify(tree, row)\n",
    "        predictions.append(int(pred))\n",
    "    return predictions\n",
    "\n",
    "##functions for validation and pruning.\n",
    "def dt_confusion_matrix( predicted, actual,classes):\n",
    "    \"\"\"Return a confusion matrix showing the difference between actual values,\n",
    "    and model predicted values. Also returns total accuracy\"\"\"\n",
    "    \n",
    "    matrix = np.zeros((len(classes), len(classes)))\n",
    "    for a, p in zip(actual, predicted):\n",
    "        matrix[a][p] += 1\n",
    "    accuracy = (actual == predicted).sum() / float(len(actual))*100\n",
    "    return matrix, accuracy        \n",
    "    \n",
    "def print_dt(tree, depth = 0):\n",
    "    \"\"\"\"Iterate through decision tree, printing out values.\"\"\"\n",
    "    print ((\" \" * depth) + \"attribute \" + str(tree['attribute']) + \" > \" + str(tree['split']))\n",
    "    if isinstance(tree['left'], dict):\n",
    "        print_dt(tree['left'], depth + 1)\n",
    "    else:\n",
    "        print ((\" \" *(depth + 1)) + str(tree['left']))\n",
    "    if isinstance(tree['right'], dict):\n",
    "        print_dt(tree['right'], depth + 1)\n",
    "    else:\n",
    "        print ((\" \" *(depth + 1)) + str(tree['right']))\n",
    "        \n",
    "\n",
    "\"\"\"Bagged decision trees contain a user-specified number of decision trees.\n",
    "Classification of a sample is done by using the mode of each of these decision trees.\n",
    "subsample is a fraction of the total dataset to be used.\n",
    "trees refers to the number of trees to use in \"forest\" of trees.\n",
    "By leaving default values for subsample and trees, a single decision tree classifier is created.\"\"\"\n",
    "def bt_train( data, max_depth, min_size = 1, subsample_ratio = 1,trees =1):\n",
    "    \n",
    "    ##Create a series of trees using sampling with replacement.\n",
    "    size = data[:, -1].size\n",
    "    division = int(size * subsample_ratio)        \n",
    "    forest = []\n",
    "    for i in range (0,trees):\n",
    "        samples = data[np.random.choice(data.shape[0], division, replace = True)]\n",
    "        forest.append([])\n",
    "        forest[i] = dt_train(samples, max_depth, min_size)\n",
    "    return forest\n",
    "\n",
    "def bt_predict( forest, data):\n",
    "    \"\"\"\"Classify validation data set based on built bagged trees.\n",
    "    This is done by taking the mode of the classifications of each decision tree.\"\"\"\n",
    "    ##Use predict function from decision tree.\n",
    "    ##Number of trees in forest, number of validation samples. Used to create empty array showing classifications.\n",
    "    forest_size = len(forest)\n",
    "    samples = len(data)\n",
    "    tree_classification = np.zeros((samples, forest_size))\n",
    "    ##With each tree, find the classification of each validation sample.\n",
    "    for i in range (0, forest_size):\n",
    "        tree_classification[:, i] = dt_predict(forest[i], data)\n",
    "    ##Create list of modes for each sample, using tree_classification matrix.\n",
    "    predictions = []\n",
    "    for i in range(0, samples):\n",
    "        tree_pred = tree_classification[i,:].tolist()\n",
    "        predictions.append(int(max(set(tree_pred), key = tree_pred.count)))\n",
    "    return predictions\n",
    "\n",
    "def bt_confusion_matrix( predicted, actual,classes):\n",
    "    \"\"\"Create confusion matrix for bagged trees. Makes call to DT method.\"\"\"\n",
    "    matrix, accuracy = dt_confusion_matrix(predicted, actual, classes)\n",
    "    return matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = np.column_stack((Xtrain ,Ytrain))\n",
    "\n",
    "test = np.column_stack((Xval ,Yval))\n",
    "print(\"start train\")\n",
    "tree = dt_train(train, 20,5)\n",
    "print(\"finish train\")\n",
    "validation_dt = dt_predict(tree, test)\n",
    "confusion_dt,accuracy_dt = dt_confusion_matrix(validation_dt, test[:, -1].astype(int), classes)\n",
    "print (accuracy_dt)\n",
    "#print (print_dt(tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start train\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-c97fc3657eef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbt_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finish train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalidation_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbt_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconfusion_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbt_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mbt_train\u001b[0;34m(data, max_depth, min_size, subsample_ratio, trees)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mforest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mdt_train\u001b[0;34m(data, max_depth, min_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msplit_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mdecision\u001b[0;34m(tree, max_depth, min_size, depth)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m##right side child. Same process as above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mdecision\u001b[0;34m(tree, max_depth, min_size, depth)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m##right side child. Same process as above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mdecision\u001b[0;34m(tree, max_depth, min_size, depth)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m##right side child. Same process as above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mdecision\u001b[0;34m(tree, max_depth, min_size, depth)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m##right side child. Same process as above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mdecision\u001b[0;34m(tree, max_depth, min_size, depth)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"attribute\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"current_mode\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mdecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m##right side child. Same process as above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mdecision\u001b[0;34m(tree, max_depth, min_size, depth)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m##Else continue building tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mleft_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0;31m##Check if node is terminal. Make it a leaf node if so.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mleft_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mleft_right\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36msplit\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muniques\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mnew_gain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_gini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m##Select the best gain, and associated attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-e1f687d945c9>\u001b[0m in \u001b[0;36mgain\u001b[0;34m(values, cur_gini, attribute, split)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mleft_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mright_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"start train\")\n",
    "forest = bt_train(train, 20, 5, 1, 50)\n",
    "print(\"finish train\")\n",
    "validation_rf = bt_predict(forest, test)\n",
    "confusion_rf, accuracy_rf = bt_confusion_matrix(validation_rf, test[:, -1].astype(int),classes)\n",
    "print (accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVENTAR +++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
