{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré processamento: \n",
    "Funções do arquivo pre_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import pre_processing as pp\n",
    "\n",
    "category = 'test'\n",
    "vocabulary = []\n",
    "X, Y, vocabulary = pp.bow(category)\n",
    "X = pp.tf_idf(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excel', 'fail', 'gentli', 'bought', ('never', 'use'), 'meat', 'rememb', 'intermitt', 'end', 'upset', 'work', ('not', 'need'), ('no', 'longer'), 'hoover', 'difficult', 'return', 'sound', 'disapoint', 'solid', 'overs', ('not', 'stick'), 'hold', 'mind', 'heard', 'stick', 'take', 'safe', 'broken', 'stuck', 'conveni', 'anymor', 'caraf', ('not', 'qualiti'), 'manual', 'base', 'apart', 'measur', 'start', 'pleas', 'ware', 'gorgeou', 'unhappi', 'sooner', 'eventu', 'check', 'quick', ('not', 'want'), 'depend', 'queen', ('not', 'good'), 'smoothi', 'begin', 'ruin', 'less', ('not', 'return'), 'add', 'experienc', ('not', 'take'), 'almost', 'receiv', ('not', 'even'), 'first', 'ergonom', 'mislead', ('never', 'worri'), 'fair', 'wet', 'live', ('not', 'happi'), 'keep', ('not', 'throw'), 'warp', 'replac', 'bake', 'instruct', 'move', 'set', 'often', 'low', ('not', 'recommend'), 'go', 'hard', 'wrinkl', 'throw', 'broke', 'fri', 'tip', 'wooden', 'older', 'tell', 'knew', 'lighter', 'spit', 'better', 'couch', 'uniqu', 'individu', 'minim', 'blue', 'poor', 'salad', 'faith', 'specif', 'warn', 'happier', 'think', 'aw', 'homemad', 'wrong', 'refund', 'lucki', 'shoddi', 'sleek', 'favorit', 'amaz', 'sit', 'uncomfort', 'quiet', 'liquid', ('not', 'function'), 'doubl', 'plastic', 'pure', 'away', 'dri', 'ever', 'tripl', 'ball', 'guess', 'chicken', 'email', 'regret', 'adjust', 'larger', 'moder', 'marin', 'enjoy', 'contact', 'probe', 'spew', 'call', 'includ', 'bread', 'feed', 'highest', 'tabl', ('not', 'turn'), 'die', 'recommend', 'wors', 'usa', 'high', 'educ', 'last', 'fabul', 'microwav', 'heavi', 'air', 'come', 'overflow', 'badli', 'thank', 'counter', 'front', ('not', 'fit'), 'excit', 'round', ('not', 'drip'), 'listen', 'fresh', ('not', 'disappoint'), 'sever', 'kept', 'serv', ('not', 'made'), 'long', 'crack', 'bunn', 'electr', 'curv', 'hassl', ('not', 'crack'), 'stove', 'asid', 'balanc', 'magic', 'practic', 'special', 'wonder', 'howev', 'stew', 'countertop', 'irregular', 'evenli', 'finger', 'smooth', ('not', 'toast'), 'gladli', 'technic', ('never', 'buy'), 'ring', 'avoid', 'happen', 'touch', 'absolut', 'cook', 'tilt', 'dinnerwar', 'wrote', 'imposs', 'read', 'well', 'whip', 'fine', 'lb', 'fool', 'fold', 'took', 'appar', 'sharpen', 'necessari', 'inferior', 'plate', 'turn', ('not', 'hesit'), 'perk', 'fast', 'thought', 'brew', 'deliv', 'shame', 'faster', 'perfect', ('not', 'smell'), 'weak', 'eleg', 'follow', 'unusu', 'easiest', 'huge', 'somewher', 'awesom', 'left', 'littermaid', ('not', 'beat'), 'got', 'concern', 'star', 'accommod', 'drawer', 'essenti', 'ahead', ('not', 'not'), ('not', 'wast'), ('not', 'well'), 'sturdi', 'frankli', 'full', 'cheap', ('not', 'fix'), 'nice', 'effici', 'foodsav', 'though', 'instead', 'panason', 'altogeth', ('not', 'sure'), 'resist', 'beat', 'appreci', 'refus', ('not', 'larg'), 'search', 'spray', 'inexpens', 'spare', 'sharp', 'chop', 'size', 'terribl', ('not', 'rust'), ('never', 'knew'), 'flimsi', 'overal', 'decid', 'comfort', ('not', 'worth'), 'disappoint', 'quickli', 'told', ('not', 'expect'), 'cast', 'thanksgiv', 'sure', 'twice', 'offer', 'exist', 'becam', 'easier', 'whisk', ('not', 'better'), 'bare', 'pill', 'resolv', ('never', 'work'), 'dutch', 'dissapoint', 'hit', 'soon', ('not', 'found'), 'warm', 'also', 'went', 'pare', 'bad', 'highli', 'shini', ('not', 'pan'), 'liter', 'advis', 'remark', 'best', 'upgrad', 'rack', 'middl', 'get', 'sent', ('not', 'enough'), 'sore', ('not', 'deliv'), 'perfectli', 'stood', 'stop', 'repair', 'prefer', ('not', 'stain'), 'healthier', 'convert', 'finish', 'ok', 'cost', 'spout', 'threw', 'half', 'fiesta', 'stain', 'larg', ('not', 'last'), 'adequ', 'fantast', 'said', ('not', 'work'), ('not', 'high'), 'clean', 'notic', 'thrill', 'black', 'ignor', 'easili', 'pancak', 'enamel', 'tri', 'correctli', 'sometim', 'horribl', 'freez', 'not', 'retain', 'unus', 'plenti', 'januari', 'ship', 'fix', 'need', ('not', 'happier'), 'funni', 'blender', 'suppos', 'unfortun', 'overfil', 'attempt', ('not', 'hear'), 'impress', 'useless', 'singl', 'track', 'littl', 'fiestawar', 'glow', 'coffe', 'con', 'purchas', 'enough', 'cup', 'sink', 'worst', ('not', 'keep'), 'properli', 'eas', 'wait', 'everywher', 'twist', 'past', ('not', 'stand'), ('not', 'order'), 'order', 'organ', 'sold', 'soft', 'distinct', 'rais', 'coat', 'delonghi', 'unplug', ('not', 'releas'), 'marri', 'fit', 'posit', 'actual', ('not', 'buy'), 'toast', 'bark', 'saucepan', ('not', 'purchas'), 'flexibl', ('not', 'regret'), 'even', 'burst', 'els', 'carri', 'compar', 'smoke', 'altern', 'particular', 'risk', 'lightweight', 'lodg', 'hardli', 'caus', 'smaller', 'easi', 'weigh', 'discov', ('not', 'wash'), 'danger', 'malfunct', 'love', 'far', 'plug', 'everyday', 'produc', 'wast', 'simpl', 'save', ('not', 'seal'), 'rival', 'catch', ('not', 'receiv'), 'frustrat', 'dead', 'gear', 'healthi', 'close', 'mayb', 'dice', 'cool', 'outstand', 'hesit', 'screw', 'smell', 'brittl', 'drive', 'lid', 'snap', 'delici', 'litter', ('not', 'burn'), 'especi', 'crush', 'awkward', 'sell', ('not', 'worri'), 'weight', 'wish', 'challeng', 'potato', 'pasta', 'lowest', 'alway', 'learn', 'manag', ('not', 'help'), 'began', 'realli', 'uneven', 'refurbish', 'second', 'stupid', 'loos', 'common', 'heed', 'poorli', ('not', 'get'), 'french', 'squar', 'scrambl', 'durabl', 'soup', 'seal', 'back', 'buy', 'warranti', 'cuisinart', 'defin', 'hope', 'ad', 'defect', 'miser', 'extra', 'fragil', 'worri', ('not', 'model'), 'handi', 'scale', 'enabl', 'addict', 'toaster', 'drain', 'requir', 'bigger', 'leak', 'beauti', 'great', 'super', 'ineffect', 'strong', 'happi', 'power', 'pass', 'continu', 'slow', 'cookwar', ('not', 'make'), 'send', 'hear']\n"
     ]
    }
   ],
   "source": [
    "X, new_vocabulary = pp.select_c2(X, Y, vocabulary)\n",
    "print(new_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holdout estratificado para separar dados para treino e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = pp.stratified_holdOut(Y, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinamento\n",
    "Xtrain = X[train_index]\n",
    "Ytrain = Y[train_index]\n",
    "\n",
    "#Validação\n",
    "Xval =  X[test_index]\n",
    "Yval = Y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-vizinhos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "K = 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cf299b3a9333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/AM/Projeto/am/k_neighbors.py\u001b[0m in \u001b[0;36mknn\u001b[0;34m(x, X, Y, K)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/AM/Projeto/am/k_neighbors.py\u001b[0m in \u001b[0;36mnormalizar\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mX_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.000000001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import k_neighbors as knn\n",
    "\n",
    "for j in range(3, 8, 2):\n",
    "    \n",
    "    K = j\n",
    "    \n",
    "    print(\"K = \" + str(K))\n",
    "    \n",
    "    classes = []\n",
    "    \n",
    "    for i in range(Xval.shape[0]):\n",
    "        y = knn.knn(Xval[i], Xtrain, Ytrain, K)\n",
    "        classes.append(y)\n",
    "        \n",
    "    acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "    print(\"Acuracia é \" + str(acuracia))    \n",
    "    \n",
    "    print(\"-------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import regressao_logistica as rl\n",
    "\n",
    "acuracia = rl.regressao_logistica(Xtrain,Ytrain,Xval,Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia é 0.7707142857142857\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import naive_bayes as nb\n",
    "\n",
    " #Com chi\n",
    "probPos, probNeg, pPos, pNeg = nb.calcularProbabilidades(pp.fp(Xtrain), Ytrain)\n",
    "classes = []\n",
    "for i in range(Xval.shape[0]):\n",
    "    y = nb.classificacao(pp.fp(Xval[i]), probPos, probNeg, pPos, pNeg)\n",
    "    classes.append(y)\n",
    "\n",
    "acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Artificiais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import redes_neurais as rn\n",
    "\n",
    "acuracia = rn.redes_neurais (Xtrain,Ytrain,Xval,Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import svm_imp as svm\n",
    "\n",
    "acuracia = svm.svm(Xtrain,Ytrain,Xval,Yval)\n",
    "print(\"Acurácia é \"+ str(acuracia))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.classify import MaxentClassifier\n",
    "\n",
    "# def word_feats(words):\n",
    "#     return dict([(word, True) for word in words])\n",
    "\n",
    "# negids = movie_reviews.fileids('neg')\n",
    "# posids = movie_reviews.fileids('pos')\n",
    "\n",
    "# negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "# posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# negcutoff = int(len(negfeats)*3/4)\n",
    "# poscutoff = int(len(posfeats)*3/4)\n",
    "\n",
    "# trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "# testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "# training_set = trainfeats\n",
    "# test_set = testfeats\n",
    "\n",
    "# #for i in range(len(Ytrain)) :\n",
    "# #    training_set.append((Xtrain_chi[i,:].tolist(), Ytrain[i]))\n",
    "# #for i in range(len(Yval)) :    \n",
    "# #    test_set.append((Xval_chi[i,:].tolist(), Yval[i]))\n",
    "\n",
    "# #def list_to_dict(words_list):\n",
    "# #  return dict([(word, True) for word in words_list])\n",
    "\n",
    "# training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in training_set] \n",
    "# numIterations = 100\n",
    " \n",
    "# algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[0]\n",
    "# classifier = nltk.MaxentClassifier.train(training_set_formatted, algorithm, max_iter=numIterations)\n",
    "# #classifier.show_most_informative_features(10)\n",
    "\n",
    "# test_set_formatted = [(list_to_dict(element[0]), element[1]) for element in test_set] \n",
    "# classes = []\n",
    "# for review in test_set_formatted:\n",
    "#     label = review[1]\n",
    "#     text = review[0]\n",
    "#     classes.append(classifier.classify(text))\n",
    "    \n",
    "# acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "# print(acuracia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "classes = clf.predict(Xval)\n",
    "acuracia = np.sum(classes==Yval)/len(Yval)\n",
    "print(\"Acurácia é \" + str(acuracia))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVENTAR +++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
